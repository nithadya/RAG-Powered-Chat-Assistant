# ğŸš€ RAG-Powered Chat Assistant

<div align="center">

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Groq](https://img.shields.io/badge/powered%20by-Groq%20Llama3-orange.svg)

**A professional Streamlit chat assistant powered by Retrieval-Augmented Generation (RAG) and Groq's Llama3 model**

*Transform your documents into intelligent conversations*

[ğŸ¯ Features](#-features) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ—ï¸ Architecture](#ï¸-architecture)

</div>

---

## ğŸ“Š Project Overview

This chat assistant combines Large Language Models with Retrieval-Augmented Generation to provide contextually aware responses. Choose between simple LLM interactions or document-powered conversations.

## ğŸ¯ Features

- ğŸ’¬ **Simple LLM Chat**: Direct communication with Llama3 via Groq API
- ğŸ” **RAG-Enhanced Chat**: Document-aware conversations with context retrieval
- ğŸ“„ **Multi-format Support**: PDF, TXT, DOCX document processing
- ğŸ’¾ **Session History**: Persistent chat conversations
- âš¡ **Fast Processing**: High-performance with Groq infrastructure
- ğŸ¨ **Modern UI**: Clean Streamlit interface

---

## ğŸ—ï¸ Project Structure

```
RAG-Powered-Chat-Assistant/
â”‚
â”œâ”€â”€ ğŸ’¬ llm_chat.py              # Simple LLM chat
â”œâ”€â”€ ğŸ” rag_chat.py              # RAG-enhanced chat
â”œâ”€â”€ ğŸ“‹ requirements.txt         # Dependencies
â”œâ”€â”€ ğŸ”§ .env                     # Environment variables
â”‚
â”œâ”€â”€ ğŸ“ data/                    # Your documents here
â”‚   â””â”€â”€ ğŸ“„ *.pdf, *.txt, *.docx
â”‚
â””â”€â”€ ğŸ“ storage/                 # Auto-generated storage
    â””â”€â”€ ğŸ—ƒï¸ vector stores & indexes
```

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites
- Python 3.8+
- Groq API key ([Get yours here](https://console.groq.com/keys))

### âš¡ Installation

1. **Clone & Setup**
   ```bash
   git clone https://github.com/your-username/RAG-Powered-Chat-Assistant.git
   cd RAG-Powered-Chat-Assistant
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # macOS/Linux
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure Environment**
   ```env
   # Create .env file
   GROQ_API_KEY=your-groq-api-key-here
   ```

### ğŸ® Usage

#### ğŸ’¬ Simple LLM Chat
```bash
streamlit run llm_chat.py
```

#### ğŸ” RAG-Enhanced Chat
1. Add documents to `data/` folder
2. Run: `streamlit run rag_chat.py`

---

## ğŸ—ï¸ System Architecture

### ğŸ”„ **Complete System Overview**

```mermaid
graph TB
    subgraph "ğŸ¯ User Interface"
        UI[ğŸ–¥ï¸ Streamlit Interface]
    end
    
    subgraph "âš™ï¸ Application Layer"
        Router[ğŸš¦ Chat Router]
        SimpleChat[ğŸ’¬ Simple Chat]
        RAGChat[ğŸ” RAG Chat]
    end
    
    subgraph "ğŸ§  AI Processing"
        LLM[ğŸ¤– Llama3 Groq]
        Embedder[ğŸ”¤ BGE Embeddings]
        Retriever[ğŸ” Context Retriever]
    end
    
    subgraph "ğŸ“Š Data Layer"
        DocLoader[ğŸ“„ Document Loader]
        VectorStore[ğŸ—ƒï¸ Vector Store]
        Storage[(ğŸ’¾ Local Storage)]
    end
    
    UI --> Router
    Router --> SimpleChat
    Router --> RAGChat
    
    SimpleChat --> LLM
    RAGChat --> Retriever
    Retriever --> VectorStore
    RAGChat --> LLM
    
    DocLoader --> Embedder
    Embedder --> VectorStore
    VectorStore --> Storage
    
    classDef ui fill:#e1f5fe
    classDef app fill:#f3e5f5
    classDef ai fill:#e8f5e8
    classDef data fill:#fff3e0
    
    class UI ui
    class Router,SimpleChat,RAGChat app
    class LLM,Embedder,Retriever ai
    class DocLoader,VectorStore,Storage data
```

### ğŸ¯ **Data Flow**

```
Simple Chat: User â†’ Streamlit â†’ Groq API â†’ Response
RAG Chat: User â†’ Streamlit â†’ Document Retrieval â†’ Context + Query â†’ Groq API â†’ Enhanced Response
```

---

## ğŸ”§ Technical Specifications

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Frontend** | Streamlit | Web interface |
| **LLM** | Llama3-8B-8192 | Text generation |
| **Embeddings** | BAAI/bge-small-en-v1.5 | Semantic search |
| **Vector Store** | LlamaIndex | Document retrieval |
| **API** | Groq | Fast inference |

### âš™ï¸ Configuration
- **Chunk Size**: 512 tokens
- **Chunk Overlap**: 20 tokens  
- **Output Limit**: 512 tokens
- **Context Window**: 8,192 tokens

---

## ğŸ“ˆ Performance

| Feature | Simple Chat | RAG Chat |
|---------|-------------|----------|
| Response Time | ~1-2s | ~3-5s |
| Context Awareness | âŒ | âœ… |
| Document Support | âŒ | âœ… |
| Accuracy | Good | Excellent |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/NewFeature`)
3. Commit changes (`git commit -m 'Add NewFeature'`)
4. Push to branch (`git push origin feature/NewFeature`)
5. Open Pull Request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Groq](https://groq.com) for fast LLM inference
- [LlamaIndex](https://llamaindex.ai) for RAG framework
- [Streamlit](https://streamlit.io) for web interface
- [HuggingFace](https://huggingface.co) for embeddings

---

<div align="center">

**â­ Star this repository if you find it helpful!**

[ğŸ› Report Bug](../../issues) â€¢ [âœ¨ Request Feature](../../issues)

Made with â¤ï¸ by [Mihisara Nithadya](https://github.com/mihisara-nithadya)

</div>
